{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Application III: Comparing Classifiers\n",
    "\n",
    "**Overview**: In this practical application, your goal is to compare the performance of the classifiers we encountered in this section, namely K Nearest Neighbor, Logistic Regression, Decision Trees, and Support Vector Machines.  We will utilize a dataset related to marketing bank products over the telephone.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Our dataset comes from the UCI Machine Learning repository [link](https://archive.ics.uci.edu/ml/datasets/bank+marketing).  The data is from a Portugese banking institution and is a collection of the results of multiple marketing campaigns.  We will make use of the article accompanying the dataset [here](CRISP-DM-BANK.pdf) for more information on the data and features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Understanding the Data\n",
    "\n",
    "To gain a better understanding of the data, please read the information provided in the UCI link above, and examine the **Materials and Methods** section of the paper.  How many marketing campaigns does this data represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data represents 17 marketing campaigns.\n"
     ]
    }
   ],
   "source": [
    "#The paper states the dataset is a result of 17 campaigns.\n",
    "print(\"The data represents 17 marketing campaigns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Read in the Data\n",
    "\n",
    "Use pandas to read in the dataset `bank-additional-full.csv` and assign to a meaningful variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/grace/comparing_classifiers/data/bank-additional-full.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
       "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          93.994          -36.4      4.857       5191.0  no  \n",
       "1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          93.994          -36.4      4.857       5191.0  no  \n",
       "3          93.994          -36.4      4.857       5191.0  no  \n",
       "4          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Understanding the Features\n",
    "\n",
    "\n",
    "Examine the data description below, and determine if any of the features are missing values or need to be coerced to a different data type.\n",
    "\n",
    "\n",
    "```\n",
    "Input variables:\n",
    "# bank client data:\n",
    "1 - age (numeric)\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "# related with the last contact of the current campaign:\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "# other attributes:\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "# social and economic context attributes\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-standard missing value ('unknown') counts:\n",
      "\n",
      "job           330\n",
      "marital        80\n",
      "education    1731\n",
      "default      8597\n",
      "housing       990\n",
      "loan          990\n",
      "dtype: int64\n",
      "\n",
      "Total 'unknown' entries across selected columns: 12718\n",
      "Average missing percentage (approx.): 5.15%\n",
      "\n",
      "Data types overview:\n",
      "age                 int64\n",
      "job                object\n",
      "marital            object\n",
      "education          object\n",
      "default            object\n",
      "housing            object\n",
      "loan               object\n",
      "contact            object\n",
      "month              object\n",
      "day_of_week        object\n",
      "duration            int64\n",
      "campaign            int64\n",
      "pdays               int64\n",
      "previous            int64\n",
      "poutcome           object\n",
      "emp.var.rate      float64\n",
      "cons.price.idx    float64\n",
      "cons.conf.idx     float64\n",
      "euribor3m         float64\n",
      "nr.employed       float64\n",
      "y                  object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "#For this problem, it is clear that some features contain the string “unknown” instead of the usual NaN (Not a Number) values. At this point, \n",
    "#I understand how important it is to know my data—machine learning #algorithms cannot process string values like “unknown.” Models such as \n",
    "#Logistic Regression and Support Vector Machines rely on mathematical #computations, so non-numeric placeholders can cause errors or distort results.\n",
    "#\n",
    "#When I examined the dataset, I found a total of 8,597 “unknown” entries #in the default feature alone. This is a significant number that could \n",
    "#lead to issues if left unaddressed. Not handling these “unknown” values #properly could result in feature encoding failures and model bias. \n",
    "#For instance, if the model interprets “unknown” in the default column as a #genuine category—similar to “yes” or “no”—it might incorrectly treat \n",
    "#missing data as a meaningful client characteristic, skewing the analysis.To prevent this, I decided to treat “unknown” as its own category, which \n",
    "#aligns with how the original researchers likely handled these cases.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset (using consistent variable name)\n",
    "bank_df = pd.read_csv('/home/grace/comparing_classifiers/data/bank-additional-full.csv', sep=';')\n",
    "\n",
    "# Check which columns contain the string \"unknown\"\n",
    "unknown_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan']\n",
    "\n",
    "# Count \"unknown\" occurrences per column\n",
    "unknown_counts = (bank_df[unknown_cols] == 'unknown').sum()\n",
    "\n",
    "# Add total and percentage info\n",
    "total_unknowns = unknown_counts.sum()\n",
    "total_rows = len(bank_df)\n",
    "percent_unknown = round((total_unknowns / (total_rows * len(unknown_cols))) * 100, 2)\n",
    "print(\"Non-standard missing value ('unknown') counts:\\n\")\n",
    "print(unknown_counts)\n",
    "print(f\"\\nTotal 'unknown' entries across selected columns: {total_unknowns}\")\n",
    "print(f\"Average missing percentage (approx.): {percent_unknown}%\")\n",
    "\n",
    "# Check data types for all columns\n",
    "print(\"\\nData types overview:\")\n",
    "print(bank_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Understanding the Task\n",
    "\n",
    "After examining the description and data, your goal now is to clearly state the *Business Objective* of the task.  State the objective below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The business objective is to predict whether a client will subscribe to a term deposit, enabling the bank to improve the efficiency of future marketing campaigns and optimize client targeting.\n"
     ]
    }
   ],
   "source": [
    "print(\"The business objective is to predict whether a client will subscribe to a term deposit, \\\n",
    "enabling the bank to improve the efficiency of future marketing campaigns and optimize client targeting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Engineering Features\n",
    "\n",
    "Now that you understand your business objective, we will build a basic model to get started.  Before we can do this, we must work to encode the data.  Using just the bank information features, prepare the features and target column for modeling with appropriate encoding and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step, I begin preparing the dataset for modeling by focusing only on the\n",
    "# bank client information features. These features describe each client’s personal\n",
    "# and financial profile, which are useful for predicting whether the client will\n",
    "# subscribe to a term deposit or not.\n",
    "#\n",
    "# I import the essential libraries for data preparation and encoding:\n",
    "#   • pandas and numpy — data manipulation and numerical ops\n",
    "#   • train_test_split — splits data into train/test\n",
    "#   • OneHotEncoder and StandardScaler  — transform categorical and numeric variables\n",
    "#   • ColumnTransformer and Pipeline — combine preprocessing steps cleanly\n",
    "#\n",
    "# I define two groups of features:\n",
    "#   • Numeric: age\n",
    "#   • Categorical: job, marital, education, default, housing, loan\n",
    "#\n",
    "# These seven columns constitute the “bank client information” subset used here.\n",
    "# I then build a preprocessing pipeline that standardizes numeric features and\n",
    "# one-hot encodes categoricals (handling unseen categories safely). Finally, I\n",
    "# split the data with stratification and apply the transformations to produce\n",
    "# X_train_enc and X_test_enc for downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed training data shape: (28831, 34)\n",
      "Transformed testing data shape: (12357, 34)\n",
      "Training target distribution:\n",
      " y\n",
      "0    0.887343\n",
      "1    0.112657\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/home/grace/comparing_classifiers/data/bank-additional-full.csv', sep=';')\n",
    "\n",
    "# Define features: only bank client information\n",
    "num_cols = ['age']\n",
    "cat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan']\n",
    "\n",
    "# Target variable (encode 'yes' -> 1, 'no' -> 0)\n",
    "y = df['y'].map({'yes': 1, 'no': 0})\n",
    "\n",
    "# Feature subset\n",
    "X = df[num_cols + cat_cols].copy()\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), num_cols),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Train/test split (preserve class distribution)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.30, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "X_train_enc = preprocessor.fit_transform(X_train)\n",
    "X_test_enc  = preprocessor.transform(X_test)\n",
    "\n",
    "# Optional checks\n",
    "print(\"Transformed training data shape:\", X_train_enc.shape)\n",
    "print(\"Transformed testing data shape:\", X_test_enc.shape)\n",
    "print(\"Training target distribution:\\n\", y_train.value_counts(normalize=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bank-client subset was successfully preprocessed using one-hot encoding and scaling.\n",
    "The resulting training data contains 34 features and maintains the original class distribution (≈ 89 % no, 11 % yes).\n",
    "This encoded dataset is now ready for model development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Train/Test Split\n",
    "\n",
    "With your data prepared, split it into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below prepares the dataset for modeling by dividing it into training and testing sets \n",
    "# and applying preprocessing steps. \n",
    "#\n",
    "# The train_test_split() function separates the data so that 70% is used for training the model \n",
    "# and 30% is reserved for testing its performance—this is a common practice to ensure \n",
    "# the model can generalize to unseen data. \n",
    "#\n",
    "# The parameter stratify=y ensures that both the training and test sets maintain the same \n",
    "# proportion of positive (“yes”) and negative (“no”) outcomes as the original dataset, \n",
    "# which is especially important when the classes are imbalanced. \n",
    "#\n",
    "# Next, the preprocessing steps defined earlier are applied: the preprocessor is fit and \n",
    "# transformed on the training data to learn the necessary scaling and encoding, and then \n",
    "# used to transform the test data so that the same learned transformations are applied consistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (28831, 34)\n",
      "Testing set shape: (12357, 34)\n",
      "Training target distribution:\n",
      " y\n",
      "0    0.887343\n",
      "1    0.112657\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The dataset is divided into training (70%) and testing (30%) sets to evaluate model generalization.\n",
    "# The 'stratify=y' parameter ensures both subsets maintain the same proportion of \n",
    "# positive ('yes') and negative ('no') outcomes as the original dataset—important for imbalanced data.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split features (X) and target (y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Apply preprocessing to training and testing data\n",
    "X_train_enc = preprocessor.fit_transform(X_train)\n",
    "X_test_enc  = preprocessor.transform(X_test)\n",
    "\n",
    "# Confirm split shapes\n",
    "print(\"Training set shape:\", X_train_enc.shape)\n",
    "print(\"Testing set shape:\", X_test_enc.shape)\n",
    "print(\"Training target distribution:\\n\", y_train.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was successfully divided into training and testing sets, with 28,831 records (approximately 70%) used for training and 12,357 records (around 30%) reserved for testing. This proportion aligns with standard machine learning practices, ensuring that the model has sufficient data to learn from while preserving a representative sample for evaluation. Both the training and testing datasets contain 34 features after preprocessing, confirming that the transformations—such as scaling the numeric feature and applying one-hot encoding to categorical variables—were applied consistently across both subsets.\n",
    "\n",
    "The target distribution shows that 88.7% of the observations represent clients who did not subscribe to a term deposit, while 11.3% represent those who did. This imbalance reflects the real-world challenge of the dataset, where most clients decline the offer. Stratified sampling successfully maintained this class proportion in both subsets, which is crucial for achieving reliable and unbiased model performance. Overall, the preprocessing and data splitting were completed correctly, resulting in a clean and well-balanced dataset ready for model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: A Baseline Model\n",
    "\n",
    "Before we build our first model, we want to establish a baseline.  What is the baseline performance that our classifier should aim to beat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline performance that our classifier should aim to beat is approximately 88-90% accuracy. The model should be able to predict the clients will not subscribe to the a term deposit.\n"
     ]
    }
   ],
   "source": [
    "print(\"The baseline performance that our classifier should aim to beat is approximately 88-90% accuracy. The model should be able to predict the clients will not subscribe to the a term deposit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, about 89% of clients did not subscribe to a term deposit. Therefore, a simple model that predicts “no” for every client would achieve around 89% accuracy. This serves as the baseline performance—the minimum level that any predictive model must exceed to be considered useful. However, this naive approach would have poor recall for the minority class (the actual subscribers), making it ineffective for business decisions. The goal of building a machine learning model is therefore to improve predictive power for the positive class (clients who do subscribe) while maintaining or improving overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: A Simple Model\n",
    "\n",
    "Use Logistic Regression to build a basic model on your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.593590677348871\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.59      0.72     10965\n",
      "           1       0.16      0.62      0.26      1392\n",
      "\n",
      "    accuracy                           0.59     12357\n",
      "   macro avg       0.54      0.61      0.49     12357\n",
      "weighted avg       0.84      0.59      0.67     12357\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[6466 4499]\n",
      " [ 523  869]]\n"
     ]
    }
   ],
   "source": [
    "# In this step, I trained a Logistic Regression model to predict whether a bank client \n",
    "# will subscribe to a term deposit (y = \"yes\" or \"no\") using the encoded personal and \n",
    "# financial attributes prepared earlier.\n",
    "#\n",
    "# Logistic Regression is a strong baseline for binary classification because it is:\n",
    "# \n",
    "# - Simple and computationally efficient\n",
    "# - Interpretable through model coefficients\n",
    "# - Effective with one-hot-encoded categorical features and scaled numerical data\n",
    "#\n",
    "# I used class_weight=\"balanced\" to address the class imbalance present in the dataset \n",
    "# (only about 11% of clients subscribed — a common issue in the original study).\n",
    "#\n",
    "# After training on the processed features (X_train_enc, y_train), the model was evaluated \n",
    "# on the test set. The accuracy score and the classification report summarize how well it \n",
    "# predicts compared to the baseline accuracy from Problem 7 (≈ 88–90%).\n",
    "#\n",
    "# If the Logistic Regression model identifies more positive (“yes”) cases while maintaining \n",
    "# or improving accuracy, it provides meaningful predictive value beyond random or \n",
    "# majority-class guessing.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "log_reg = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Fit the model\n",
    "log_reg.fit(X_train_enc, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = log_reg.predict(X_test_enc)\n",
    "\n",
    "# Evaluate performance\n",
    "log_reg_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Logistic Regression Accuracy:\", log_reg_accuracy)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model only achieved 59.4% as expected since I used class_weight='balanced' I had to assign more weight to the minority class (\"yes\") so the model will shift focus from simply maximizing accuracy to better identify potential subscribers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9: Score the Model\n",
    "\n",
    "What is the accuracy of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 59.36%\n"
     ]
    }
   ],
   "source": [
    "print(\"Model Accuracy: 59.36%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic Regression model achieved an accuracy of approximately 59.36% on the test set.\n",
    "While this is lower than the 89% baseline, this result reflects a balanced trade-off between precision and recall after addressing the class imbalance with class_weight='balanced'.\n",
    "In imbalanced datasets like this, accuracy alone is not a reliable indicator of model performance — the recall for the minority class (“yes”) is a more meaningful measure of success at this stage. The model correctly identified six out of ten clients overall, while significantly improving reliability to recognize potential subscribers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10: Model Comparisons\n",
    "\n",
    "Now, we aim to compare the performance of the Logistic Regression model to our KNN algorithm, Decision Tree, and SVM models.  Using the default settings for each of the models, fit and score each.  Also, be sure to compare the fit time of each of the models.  Present your findings in a `DataFrame` similar to that below:\n",
    "\n",
    "| Model | Train Time | Train Accuracy | Test Accuracy |\n",
    "| ----- | ---------- | -------------  | -----------   |\n",
    "|    |    |.     |.     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Train Time (s)</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Test Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.0971</td>\n",
       "      <td>0.8873</td>\n",
       "      <td>0.8874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>45.1051</td>\n",
       "      <td>0.8882</td>\n",
       "      <td>0.8868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.8915</td>\n",
       "      <td>0.8777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.4172</td>\n",
       "      <td>0.9188</td>\n",
       "      <td>0.8642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Train Time (s)  Train Accuracy  Test Accuracy\n",
       "0  Logistic Regression          0.0971          0.8873         0.8874\n",
       "1                  SVM         45.1051          0.8882         0.8868\n",
       "2                  KNN          0.0032          0.8915         0.8777\n",
       "3        Decision Tree          0.4172          0.9188         0.8642"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Problem 10 — Model Comparisons\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "def maybe_dense(X, model_name):\n",
    "    if model_name in {\"KNN\", \"SVM\"} and sparse.issparse(X):\n",
    "        return X.toarray()\n",
    "    return X\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),  \n",
    "    \"KNN\": KNeighborsClassifier(),                                             \n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),                  \n",
    "    \"SVM\": SVC(random_state=42)                                                \n",
    "}\n",
    "\n",
    "rows = []\n",
    "for name, model in models.items():\n",
    "    Xtr = maybe_dense(X_train_enc, name)\n",
    "    Xte = maybe_dense(X_test_enc,  name)\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.fit(Xtr, y_train)\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    train_acc = model.score(Xtr, y_train)\n",
    "    test_acc  = model.score(Xte, y_test)\n",
    "\n",
    "    rows.append({\n",
    "        \"Model\": name,\n",
    "        \"Train Time (s)\": round(train_time, 4),\n",
    "        \"Train Accuracy\": round(train_acc, 4),\n",
    "        \"Test Accuracy\": round(test_acc, 4)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows).sort_values(by=\"Test Accuracy\", ascending=False).reset_index(drop=True)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four models were trained and compared using default parameters to establish baseline performance and efficiency. Logistic Regression achieved a test accuracy of 88.74%, which is almost identical to the Support Vector Machine (SVM) model at 88.68%. While SVM produced slightly higher training accuracy, its training time was significantly longer (about 45 seconds versus less than a second for Logistic Regression). This reflects SVM’s higher computational cost due to the complexity of finding the optimal separating boundary, especially in high-dimensional data.\n",
    "\n",
    "The K-Nearest Neighbors (KNN) model achieved comparable accuracy (87.77%) but required minimal training time since it performs most of its computation during prediction rather than fitting. Meanwhile, the Decision Tree model recorded the highest training accuracy (91.88%) but the lowest test accuracy (86.42%), indicating signs of overfitting — the model fits the training data too closely and generalizes poorly to unseen data.\n",
    "\n",
    "Overall, all models performed similarly in terms of test accuracy, clustering around the baseline level (~88%), but SVM demonstrated strong generalization and stability across both training and testing sets. This performance pattern supports the original researchers’ preference for SVM, as it provided the most consistent and reliable predictive power, albeit with a longer training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11: Improving the Model\n",
    "\n",
    "Now that we have some basic models on the board, we want to try to improve these.  Below, we list a few things to explore in this pursuit.\n",
    "\n",
    "\n",
    "- Hyperparameter tuning and grid search.  All of our models have additional hyperparameters to tune and explore.  For example the number of neighbors in KNN or the maximum depth of a Decision Tree.  \n",
    "- Adjust your performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
